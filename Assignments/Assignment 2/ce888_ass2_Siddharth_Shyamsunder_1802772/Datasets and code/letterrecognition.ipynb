{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report \n",
    "from sklearn import svm,preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the training dataset: 340000\n"
     ]
    }
   ],
   "source": [
    "data2=open('letter-recognition.data','r',encoding='utf-8')\n",
    "data2=data2.read()\n",
    "#Reading data from the dataset\n",
    "data2=\",\".join(data2.splitlines())\n",
    "\n",
    "data2=data2.split(',')\n",
    "\n",
    "data4=[]\n",
    "\n",
    "count=len(data2)\n",
    "print('Length of the training dataset:',count)\n",
    "for i in range(count//17):\n",
    "    data4.append([])\n",
    "i=0\n",
    "\n",
    "for j in range(0,(count)):\n",
    "    if(j%17==0 and j!=0):\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "    data4[i].append(data2[j])\n",
    "\n",
    "#Copying Dataframe into csv \n",
    "data=\"lettr,x-box,y-box,width,height,onpix,x-bar,y-bar,x2bar,y2bar,xybar,x2ybr,xy2br,x-ege,xegvy,y-ege,yegvx\".split(',')\n",
    "csvdata=data\n",
    "with open(\"letterrec.csv\",'w',encoding='utf-8') as csvFile:\n",
    "    writer=csv.writer(csvFile)\n",
    "    writer.writerow(csvdata)\n",
    "csvFile.close()\n",
    "\n",
    "data=data4\n",
    "csvdata=data\n",
    "with open(\"letterrec.csv\",'a',encoding='utf-8') as csvFile:\n",
    "    writer=csv.writer(csvFile)\n",
    "    writer.writerows(csvdata)\n",
    "csvFile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x-box  y-box  width  height  onpix  x-bar  y-bar  x2bar  y2bar  xybar  \\\n",
      "0      2      8      3       5      1      8     13      0      6      6   \n",
      "1      5     12      3       7      2     10      5      5      4     13   \n",
      "2      4     11      6       8      6     10      6      2      6     10   \n",
      "3      7     11      6       6      3      5      9      4      6      4   \n",
      "4      2      1      3       1      1      8      6      6      6      6   \n",
      "\n",
      "   x2ybr  xy2br  x-ege  xegvy  y-ege  yegvx  \n",
      "0     10      8      0      8      0      8  \n",
      "1      3      9      2      8      4     10  \n",
      "2      3      7      3      7      3      9  \n",
      "3      4     10      6     10      2      8  \n",
      "4      5      9      1      7      5     10  \n"
     ]
    }
   ],
   "source": [
    "#Reading Data Frame\n",
    "data2=pd.read_csv('letterrec.csv')\n",
    "data2=data2.dropna()\n",
    "Y=data2['lettr']\n",
    "encoder=LabelEncoder()\n",
    "Y=encoder.fit_transform(data2.iloc[:,0].astype(str))\n",
    "X=data2.drop('lettr',axis=1)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Learning data with 80% Training and 20% Testing data using Random Forest Method\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,train_size = 0.8,test_size=0.2,random_state=0)\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random forest prediction vs actual tested reading: 0.9725\n",
      "The confusion Matrix is as Below:\n",
      "\n",
      "[[147   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0 152   0   0   0   0   2   0   0   0   1   0   0   0   0   0   0   1\n",
      "    1   0   0   1   0   0   0   0]\n",
      " [  0   0 156   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  1   0   0 169   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   1   0]\n",
      " [  0   0   1   0 141   0   0   0   0   0   0   2   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   2   0 158   0   0   0   0   0   0   0   1   1   2   0   0\n",
      "    0   1   0   0   1   0   0   0]\n",
      " [  0   1   0   2   1   0 173   0   0   0   0   0   0   0   0   0   3   1\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [  0   1   0   1   0   0   0 117   0   0   2   0   0   0   0   0   0   1\n",
      "    0   0   1   0   0   0   0   0]\n",
      " [  0   1   0   1   0   1   0   0 119   5   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   4 152   0   0   0   0   0   0   1   0\n",
      "    0   1   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   2   0   0 134   0   0   0   0   0   0   5\n",
      "    0   0   0   0   0   1   0   0]\n",
      " [  0   1   0   0   1   0   0   0   0   0   0 156   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   0 169   0   0   0   0   0\n",
      "    0   0   0   2   1   0   0   0]\n",
      " [  0   0   0   1   0   0   0   1   0   0   0   0   1 131   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   2   0   0   0   0   0   0   0   0   0   0 138   0   1   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   1   3   0   0   0   0   0   0   0   0   0 159   0   0\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 144   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   3   0   0   0 145\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   1   0   0   0   0   0   0   1   0   0   0   0   1   0\n",
      "  150   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1 173   1   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   0   0   1   0   1   0   0   0\n",
      "    0   0 157   0   0   0   0   0]\n",
      " [  0   3   0   0   0   1   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0   0 147   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0   1   0 139   0   0   0]\n",
      " [  0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0 170   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1   1   0   0   0 152   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      1.00      1.00       147\n",
      "           B       0.93      0.96      0.94       158\n",
      "           C       0.99      1.00      0.99       156\n",
      "           D       0.93      0.99      0.96       171\n",
      "           E       0.97      0.97      0.97       145\n",
      "           F       0.96      0.95      0.95       167\n",
      "           G       0.98      0.95      0.97       182\n",
      "           H       0.97      0.95      0.96       123\n",
      "           I       0.97      0.94      0.95       127\n",
      "           J       0.97      0.96      0.96       159\n",
      "           K       0.98      0.94      0.96       143\n",
      "           L       0.98      0.98      0.98       159\n",
      "           M       0.98      0.98      0.98       173\n",
      "           N       0.97      0.98      0.97       134\n",
      "           O       0.99      0.97      0.98       142\n",
      "           P       0.99      0.96      0.98       165\n",
      "           Q       0.94      0.99      0.97       145\n",
      "           R       0.94      0.97      0.96       149\n",
      "           S       0.99      0.97      0.98       154\n",
      "           T       0.98      0.98      0.98       177\n",
      "           U       0.98      0.98      0.98       160\n",
      "           V       0.97      0.96      0.96       153\n",
      "           W       0.99      0.99      0.99       141\n",
      "           X       0.99      0.98      0.99       173\n",
      "           Y       0.98      0.99      0.98       154\n",
      "           Z       1.00      0.99      1.00       143\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      4000\n",
      "   macro avg       0.97      0.97      0.97      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Confusion Matrix and Classification Evaluation Report after Random Forest Learning\n",
    "ypred = clf.predict(x_test)\n",
    "Y1=list(encoder.inverse_transform(y_test))\n",
    "Y2=list(encoder.inverse_transform(ypred))\n",
    "print(\"Accuracy of Random forest prediction vs actual tested reading:\",accuracy_score(y_test, ypred))\n",
    "print('The confusion Matrix is as Below:\\n')\n",
    "print(confusion_matrix(Y1,Y2,labels=list(encoder.classes_)))\n",
    "print(classification_report(Y1,Y2,target_names=list(encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Probability Fields using Binning Techniques\n",
    "yproba=clf.predict_proba(X)\n",
    "\n",
    "binlab=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "bins=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x-box  y-box  width  height  onpix  x-bar  y-bar  x2bar  y2bar  xybar  ...  \\\n",
      "0      2      8      3       5      1      8     13      0      6      6  ...   \n",
      "1      5     12      3       7      2     10      5      5      4     13  ...   \n",
      "2      4     11      6       8      6     10      6      2      6     10  ...   \n",
      "3      7     11      6       6      3      5      9      4      6      4  ...   \n",
      "4      2      1      3       1      1      8      6      6      6      6  ...   \n",
      "\n",
      "   p17  p18  p19  p20  p21  p22  p23  p24  p25  p26  \n",
      "0  0.1  0.1  0.1  1.0  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "2  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "3  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "4  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Adding Probability fields to the entire feature dataset. \n",
    "X['p1']=pd.cut(yproba[:,0],bins,labels=binlab, include_lowest=True)\n",
    "X['p2']=pd.cut(yproba[:,1],bins,labels=binlab, include_lowest=True)\n",
    "X['p3']=pd.cut(yproba[:,2],bins,labels=binlab, include_lowest=True)\n",
    "X['p4']=pd.cut(yproba[:,3],bins,labels=binlab, include_lowest=True)\n",
    "X['p5']=pd.cut(yproba[:,4],bins,labels=binlab, include_lowest=True)\n",
    "X['p6']=pd.cut(yproba[:,5],bins,labels=binlab, include_lowest=True)\n",
    "X['p7']=pd.cut(yproba[:,6],bins,labels=binlab, include_lowest=True)\n",
    "X['p8']=pd.cut(yproba[:,7],bins,labels=binlab, include_lowest=True)\n",
    "X['p9']=pd.cut(yproba[:,8],bins,labels=binlab, include_lowest=True)\n",
    "X['p10']=pd.cut(yproba[:,9],bins,labels=binlab, include_lowest=True)\n",
    "X['p11']=pd.cut(yproba[:,10],bins,labels=binlab, include_lowest=True)\n",
    "X['p12']=pd.cut(yproba[:,11],bins,labels=binlab, include_lowest=True)\n",
    "X['p13']=pd.cut(yproba[:,12],bins,labels=binlab, include_lowest=True)\n",
    "X['p14']=pd.cut(yproba[:,13],bins,labels=binlab, include_lowest=True)\n",
    "X['p15']=pd.cut(yproba[:,14],bins,labels=binlab, include_lowest=True)\n",
    "X['p16']=pd.cut(yproba[:,15],bins,labels=binlab, include_lowest=True)\n",
    "X['p17']=pd.cut(yproba[:,16],bins,labels=binlab, include_lowest=True)\n",
    "X['p18']=pd.cut(yproba[:,17],bins,labels=binlab, include_lowest=True)\n",
    "X['p19']=pd.cut(yproba[:,18],bins,labels=binlab, include_lowest=True)\n",
    "X['p20']=pd.cut(yproba[:,19],bins,labels=binlab, include_lowest=True)\n",
    "X['p21']=pd.cut(yproba[:,20],bins,labels=binlab, include_lowest=True)\n",
    "X['p22']=pd.cut(yproba[:,21],bins,labels=binlab, include_lowest=True)\n",
    "X['p23']=pd.cut(yproba[:,22],bins,labels=binlab, include_lowest=True)\n",
    "X['p24']=pd.cut(yproba[:,23],bins,labels=binlab, include_lowest=True)\n",
    "X['p25']=pd.cut(yproba[:,24],bins,labels=binlab, include_lowest=True)\n",
    "X['p26']=pd.cut(yproba[:,25],bins,labels=binlab, include_lowest=True)\n",
    "\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dividing the entire dataset into training and test data and performing Decision Tree classification.\n",
    "xtr1, xte1, ytr1, yte1 = train_test_split(X,Y,train_size = 0.8,test_size=0.2,random_state=0)\n",
    "dt = DecisionTreeClassifier(min_samples_split=100)\n",
    "dt.fit(xtr1,ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred1 = dt.predict(xte1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of using decision tree after random forest: 0.9115\n",
      "The confusion Matrix is as Below:\n",
      "\n",
      "[[142   0   0   0   0   0   0   0   0   5   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0 127   0   0   0   0   0   0   0  30   0   0   0   0   0   0   0   0\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [  0   0 154   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 142   0   0   0   0   0  28   0   0   0   0   1   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0 129   0   0   0   0  13   0   1   0   0   0   0   1   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0 150   0   0   0  16   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0 159   0   0  22   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  91   0  30   2   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 116  11   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0   2 156   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0  12 129   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8   0 151   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   9   0   0 164   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  10   0   0   0 124   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   6   0   0   0   0 134   0   1   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0   0  13   0   0   0   0   0 150   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   5   0   0   0   0   0   0 139   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  23   0   0   0   1   0   0   0 125\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0  13   0   0   0   0   0   0   0   0\n",
      "  140   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0   0\n",
      "    0 165   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12   0   0   1   0   0   0   0   0\n",
      "    0   0 147   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  11   0   0   0   0   0   0   0   0\n",
      "    0   0   0 141   0   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0  12   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 129   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  20   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0 153   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0 146   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.97      0.98       147\n",
      "           B       0.97      0.80      0.88       158\n",
      "           C       0.99      0.99      0.99       156\n",
      "           D       1.00      0.83      0.91       171\n",
      "           E       0.99      0.89      0.94       145\n",
      "           F       0.98      0.90      0.94       167\n",
      "           G       1.00      0.87      0.93       182\n",
      "           H       1.00      0.74      0.85       123\n",
      "           I       0.98      0.91      0.95       127\n",
      "           J       0.32      0.98      0.48       159\n",
      "           K       0.98      0.90      0.94       143\n",
      "           L       0.99      0.95      0.97       159\n",
      "           M       0.99      0.95      0.97       173\n",
      "           N       0.99      0.93      0.96       134\n",
      "           O       0.99      0.94      0.97       142\n",
      "           P       1.00      0.91      0.95       165\n",
      "           Q       0.99      0.96      0.97       145\n",
      "           R       0.99      0.84      0.91       149\n",
      "           S       1.00      0.91      0.95       154\n",
      "           T       1.00      0.93      0.96       177\n",
      "           U       1.00      0.92      0.96       160\n",
      "           V       0.99      0.92      0.96       153\n",
      "           W       1.00      0.91      0.96       141\n",
      "           X       1.00      0.88      0.94       173\n",
      "           Y       0.99      0.95      0.97       154\n",
      "           Z       1.00      1.00      1.00       143\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      4000\n",
      "   macro avg       0.97      0.91      0.93      4000\n",
      "weighted avg       0.97      0.91      0.93      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating Accuracy, Confusion Matrix and Classification Report on Decision Tree After distillation of Random Forest \n",
    "print(\"Accuracy of using decision tree after random forest:\",accuracy_score(yte1, ypred1))\n",
    "Y3=list(encoder.inverse_transform(yte1))\n",
    "Y4=list(encoder.inverse_transform(ypred1))\n",
    "print('The confusion Matrix is as Below:\\n')\n",
    "print(confusion_matrix(Y3,Y4,labels=list(encoder.classes_)))\n",
    "print(classification_report(Y3,Y4,target_names=list(encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trying other Techniques after distillation namely Support Vector Machine\n",
    "xtr1, xte1, ytr1, yte1 = train_test_split(X,Y,train_size = 0.8,test_size=0.2,random_state=0)\n",
    "support = svm.SVC(gamma=0.01, C=100.)\n",
    "support.fit(xtr1,ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred1 = support.predict(xte1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of using Support Vector Machine after random forest: 0.98175\n",
      "The confusion Matrix is as Below:\n",
      "\n",
      "[[145   0   0   1   0   0   0   0   0   1   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0 152   0   0   1   0   0   0   0   0   1   0   0   0   0   0   0   2\n",
      "    1   0   0   1   0   0   0   0]\n",
      " [  0   0 156   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 167   0   0   2   1   0   0   0   0   0   0   0   0   0   0\n",
      "    1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 143   0   0   0   0   0   0   2   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 162   1   1   0   0   0   0   0   0   0   2   0   0\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   1   1   0 178   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 120   0   0   1   0   0   0   0   0   0   1\n",
      "    0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 122   4   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   6 153   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   2   0   0 136   0   0   0   0   0   0   3\n",
      "    0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   1   0   1   1   0   0   0 156   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   0   0 172   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1 133   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   1   1   0   0   1   0   0   0   0   0   0   0 136   0   2   0\n",
      "    0   0   1   0   0   0   0   0]\n",
      " [  0   0   0   0   1   3   0   0   0   0   0   0   0   0   0 161   0   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 145   0\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0 147\n",
      "    0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  154   0   0   0   0   0   0   0]\n",
      " [  0   1   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0\n",
      "    0 174   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0 158   0   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0 151   1   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0 139   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "    0   0   0   0   0 172   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   1   0   0   0   1 152   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0 143]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      0.99      0.99       147\n",
      "           B       0.98      0.96      0.97       158\n",
      "           C       0.99      1.00      0.99       156\n",
      "           D       0.98      0.98      0.98       171\n",
      "           E       0.97      0.99      0.98       145\n",
      "           F       0.98      0.97      0.97       167\n",
      "           G       0.97      0.98      0.98       182\n",
      "           H       0.95      0.98      0.96       123\n",
      "           I       0.95      0.96      0.96       127\n",
      "           J       0.97      0.96      0.97       159\n",
      "           K       0.98      0.95      0.96       143\n",
      "           L       0.98      0.98      0.98       159\n",
      "           M       0.98      0.99      0.99       173\n",
      "           N       0.99      0.99      0.99       134\n",
      "           O       1.00      0.96      0.98       142\n",
      "           P       0.99      0.98      0.98       165\n",
      "           Q       0.99      1.00      0.99       145\n",
      "           R       0.95      0.99      0.97       149\n",
      "           S       0.99      1.00      0.99       154\n",
      "           T       0.99      0.98      0.99       177\n",
      "           U       0.99      0.99      0.99       160\n",
      "           V       0.98      0.99      0.98       153\n",
      "           W       0.99      0.99      0.99       141\n",
      "           X       0.99      0.99      0.99       173\n",
      "           Y       1.00      0.99      0.99       154\n",
      "           Z       1.00      1.00      1.00       143\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      4000\n",
      "   macro avg       0.98      0.98      0.98      4000\n",
      "weighted avg       0.98      0.98      0.98      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of using Support Vector Machine after random forest:\",accuracy_score(yte1, ypred1))\n",
    "Y5=list(encoder.inverse_transform(yte1))\n",
    "Y6=list(encoder.inverse_transform(ypred1))\n",
    "print('The confusion Matrix is as Below:\\n')\n",
    "print(confusion_matrix(Y5,Y6,labels=list(encoder.classes_)))\n",
    "print(classification_report(Y5,Y6,target_names=list(encoder.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making use of training data without distillation using single not ensemble method like Decision Tree.\n",
    "dt1=DecisionTreeClassifier(min_samples_split=100)\n",
    "dt1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = dt1.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree prediction without distillation: 0.7515\n",
      "The confusion Matrix is as Below:\n",
      "\n",
      "[[120   0   1   1   4   0   0   1   0   0   0   1   1   1   0   0   1   0\n",
      "   13   0   2   0   0   1   0   0]\n",
      " [  0 104   0   8   1   2   0   7   4   0   3   0   0   2   1   3   1  14\n",
      "    1   0   0   4   0   0   3   0]\n",
      " [  0   0 137   0   1   3   1   0   2   0   1   0   0   0   1   1   2   2\n",
      "    0   0   0   1   0   0   4   0]\n",
      " [  0   9   0 135   1   1   0   2   0   0   1   0   0   3   1   3   0  10\n",
      "    5   0   0   0   0   0   0   0]\n",
      " [  0   3   3   0  92   1   2   2   0   0   8   7   0   0   1   0   7   1\n",
      "   12   3   0   0   0   1   1   1]\n",
      " [  0   5   0   2   0 117   1   1   2   0   0   0   0   6   1  16   0   1\n",
      "    0   0   0   0   1   0  13   1]\n",
      " [  0   7   5   1  10   3 127   1   0   0   4   0   0   0   3   3   4   4\n",
      "    2   1   1   4   1   1   0   0]\n",
      " [  0   3   0   3   1   2   0  74   0   0   3   0   0   5  11   4   0   9\n",
      "    0   0   0   2   0   4   2   0]\n",
      " [  0   3   1   4   0   0   1   3 100   1   0   0   0   0   0   1   2   4\n",
      "    5   0   0   0   0   2   0   0]\n",
      " [  0   3   2   5   0   0   0   5   5 119   2   0   0   1   3   5   2   4\n",
      "    1   0   1   0   0   1   0   0]\n",
      " [  0   0   0   1   3   1   5   2   0   0 107   4   1   0   0   0   0  12\n",
      "    2   0   0   0   0   5   0   0]\n",
      " [  0   0   4   0   0   1   2   0   0   0   1 134   0   3   2   0   0   2\n",
      "    8   0   0   0   0   2   0   0]\n",
      " [  1   0   0   2   0   3   2   3   0   0   0   1 141   7   0   0   0   3\n",
      "    0   1   1   3   5   0   0   0]\n",
      " [  0   0   0   0   0   0   0   3   0   0   0   1   1 113   1   0   0   5\n",
      "    1   0   1   3   1   0   4   0]\n",
      " [  0   3   1   2   1   0   8   6   0   0   1   1   0   0  85   2  18   9\n",
      "    0   0   1   0   3   1   0   0]\n",
      " [  0   1   0   2   5  10   1   0   3   2   0   0   0   1   1 131   0   0\n",
      "    1   3   0   2   1   0   1   0]\n",
      " [  0   2   0   0   1   0   2   5   0   1   8   2   0   0   2   1 103   4\n",
      "    0   2   1   5   0   4   1   1]\n",
      " [  0   7   0   6   0   0   1   3   0   0   2   0   0   4   1   0   0 119\n",
      "    5   0   0   0   0   0   1   0]\n",
      " [  0  13   0   6   2   3   3   2   1   1   5   2   0   0   0   4   5   4\n",
      "   96   0   0   1   0   2   0   4]\n",
      " [  0   0   0   2   0   5   3   1   0   0   6   1   0   0   0   7   1   3\n",
      "    1 137   0   0   0   4   6   0]\n",
      " [  0   0   0   0   2   0   2   6   0   0   5   0   6   6   2   0   2   3\n",
      "    0   0 118   0   3   1   4   0]\n",
      " [  0   0   0   0   0   1   1   1   0   0   2   0   0   2   0   0   0   3\n",
      "    0   6   0 128   3   0   6   0]\n",
      " [  0   0   0   0   0   0   0   7   0   0   0   0   4   1   4   0   2   1\n",
      "    0   0   2   2 115   0   3   0]\n",
      " [  0   2   0   6   4   4   3   4   0   0   3   2   0   0   1   1   0   5\n",
      "    7   0   0   0   0 129   2   0]\n",
      " [  0   0   0   7   0   0   0   3   0   0   0   0   0   0   0   3   1   0\n",
      "    2   7   4   8   0   0 118   1]\n",
      " [  0   4   0   1  10   2   2   2   0   0   0   1   0   0   0   1   0   4\n",
      "    4   0   0   0   0   0   5 107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      0.82      0.90       147\n",
      "           B       0.62      0.66      0.64       158\n",
      "           C       0.89      0.88      0.88       156\n",
      "           D       0.70      0.79      0.74       171\n",
      "           E       0.67      0.63      0.65       145\n",
      "           F       0.74      0.70      0.72       167\n",
      "           G       0.76      0.70      0.73       182\n",
      "           H       0.51      0.60      0.55       123\n",
      "           I       0.85      0.79      0.82       127\n",
      "           J       0.96      0.75      0.84       159\n",
      "           K       0.66      0.75      0.70       143\n",
      "           L       0.85      0.84      0.85       159\n",
      "           M       0.92      0.82      0.86       173\n",
      "           N       0.73      0.84      0.78       134\n",
      "           O       0.70      0.60      0.65       142\n",
      "           P       0.70      0.79      0.75       165\n",
      "           Q       0.68      0.71      0.70       145\n",
      "           R       0.53      0.80      0.63       149\n",
      "           S       0.58      0.62      0.60       154\n",
      "           T       0.86      0.77      0.81       177\n",
      "           U       0.89      0.74      0.81       160\n",
      "           V       0.79      0.84      0.81       153\n",
      "           W       0.86      0.82      0.84       141\n",
      "           X       0.82      0.75      0.78       173\n",
      "           Y       0.68      0.77      0.72       154\n",
      "           Z       0.93      0.75      0.83       143\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      4000\n",
      "   macro avg       0.76      0.75      0.75      4000\n",
      "weighted avg       0.77      0.75      0.76      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y7=list(encoder.inverse_transform(y_test))\n",
    "Y8=list(encoder.inverse_transform(ypred))\n",
    "print(\"Accuracy of Decision Tree prediction without distillation:\",accuracy_score(y_test, ypred))\n",
    "print('The confusion Matrix is as Below:\\n')\n",
    "print(confusion_matrix(Y7,Y8,labels=list(encoder.classes_)))\n",
    "print(classification_report(Y7,Y8,target_names=list(encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x-box  y-box  width  height  onpix  x-bar  y-bar  x2bar  y2bar  xybar  ...  \\\n",
      "0      2      8      3       5      1      8     13      0      6      6  ...   \n",
      "1      5     12      3       7      2     10      5      5      4     13  ...   \n",
      "2      4     11      6       8      6     10      6      2      6     10  ...   \n",
      "3      7     11      6       6      3      5      9      4      6      4  ...   \n",
      "4      2      1      3       1      1      8      6      6      6      6  ...   \n",
      "\n",
      "   p17  p18  p19  p20  p21  p22  p23  p24  p25  p26  \n",
      "0  0.1  0.1  0.1  1.0  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "1  0.4  0.1  0.2  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "2  0.1  0.8  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "3  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "4  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# Adding Probability fields to the entire feature dataset and binning. \n",
    "X1=data2.drop('lettr',axis=1)\n",
    "yproba1=dt1.predict_proba(X1)\n",
    "\n",
    "binlab=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "bins=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "\n",
    "X1['p1']=pd.cut(yproba1[:,0],bins,labels=binlab, include_lowest=True)\n",
    "X1['p2']=pd.cut(yproba1[:,1],bins,labels=binlab, include_lowest=True)\n",
    "X1['p3']=pd.cut(yproba1[:,2],bins,labels=binlab, include_lowest=True)\n",
    "X1['p4']=pd.cut(yproba1[:,3],bins,labels=binlab, include_lowest=True)\n",
    "X1['p5']=pd.cut(yproba1[:,4],bins,labels=binlab, include_lowest=True)\n",
    "X1['p6']=pd.cut(yproba1[:,5],bins,labels=binlab, include_lowest=True)\n",
    "X1['p7']=pd.cut(yproba1[:,6],bins,labels=binlab, include_lowest=True)\n",
    "X1['p8']=pd.cut(yproba1[:,7],bins,labels=binlab, include_lowest=True)\n",
    "X1['p9']=pd.cut(yproba1[:,8],bins,labels=binlab, include_lowest=True)\n",
    "X1['p10']=pd.cut(yproba1[:,9],bins,labels=binlab, include_lowest=True)\n",
    "X1['p11']=pd.cut(yproba1[:,10],bins,labels=binlab, include_lowest=True)\n",
    "X1['p12']=pd.cut(yproba1[:,11],bins,labels=binlab, include_lowest=True)\n",
    "X1['p13']=pd.cut(yproba1[:,12],bins,labels=binlab, include_lowest=True)\n",
    "X1['p14']=pd.cut(yproba1[:,13],bins,labels=binlab, include_lowest=True)\n",
    "X1['p15']=pd.cut(yproba1[:,14],bins,labels=binlab, include_lowest=True)\n",
    "X1['p16']=pd.cut(yproba1[:,15],bins,labels=binlab, include_lowest=True)\n",
    "X1['p17']=pd.cut(yproba1[:,16],bins,labels=binlab, include_lowest=True)\n",
    "X1['p18']=pd.cut(yproba1[:,17],bins,labels=binlab, include_lowest=True)\n",
    "X1['p19']=pd.cut(yproba1[:,18],bins,labels=binlab, include_lowest=True)\n",
    "X1['p20']=pd.cut(yproba1[:,19],bins,labels=binlab, include_lowest=True)\n",
    "X1['p21']=pd.cut(yproba1[:,20],bins,labels=binlab, include_lowest=True)\n",
    "X1['p22']=pd.cut(yproba1[:,21],bins,labels=binlab, include_lowest=True)\n",
    "X1['p23']=pd.cut(yproba1[:,22],bins,labels=binlab, include_lowest=True)\n",
    "X1['p24']=pd.cut(yproba1[:,23],bins,labels=binlab, include_lowest=True)\n",
    "X1['p25']=pd.cut(yproba1[:,24],bins,labels=binlab, include_lowest=True)\n",
    "X1['p26']=pd.cut(yproba1[:,25],bins,labels=binlab, include_lowest=True)\n",
    "\n",
    "print(X1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Decision Tree after Distillation using individual Learning like Decision Tree\n",
    "xtr1, xte1, ytr1, yte1 = train_test_split(X1,Y,train_size = 0.8,test_size=0.2,random_state=0)\n",
    "dt2 = DecisionTreeClassifier(min_samples_split=100)\n",
    "dt2.fit(xtr1,ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred2 = dt2.predict(xte1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of using decision tree after distillation using decision tree: 0.79425\n",
      "The confusion Matrix is as Below:\n",
      "\n",
      "[[125   1   1   0   0   0   4   1   0   0   1   6   1   0   1   0   0   0\n",
      "    1   0   0   0   0   1   2   2]\n",
      " [  0  99   0   5   1   3   4   7   2   0   7   1   0   1   2   5   1  12\n",
      "    0   1   0   0   0   3   3   1]\n",
      " [  0   0 138   2   0   5   1   3   0   0   0   0   0   0   1   3   2   0\n",
      "    0   1   0   0   0   0   0   0]\n",
      " [  0   8   0 140   1   2   0   5   0   0   2   4   0   1   0   3   0   3\n",
      "    1   0   1   0   0   0   0   0]\n",
      " [  0   0   3   1 109   2   6   1   0   0   3   0   0   0   0   2   4   1\n",
      "    3   3   0   0   0   3   0   4]\n",
      " [  0   3   0   0   2 126   1   0   0   5   1   0   0   2   0  12   1   0\n",
      "    1   1   0   0   1   0  10   1]\n",
      " [  0   8   4   3   2   0 136   3   0   0   1   3   0   0   3   3   5   3\n",
      "    3   0   0   0   3   1   1   0]\n",
      " [  0   2   0   3   0   0   1  87   0   0   5   0   0   1   3   4   0  10\n",
      "    0   0   3   0   1   3   0   0]\n",
      " [  0   1   1   0   0   1   0   1 106   4   0   1   0   1   0   0   0   0\n",
      "    8   0   0   0   0   2   0   1]\n",
      " [  0   1   2   3   1   0   0   3   4 128   0   0   0   1   3   1   3   3\n",
      "    2   0   0   0   0   3   1   0]\n",
      " [  1   1   0   1   1   0   7   6   1   0 109   2   1   0   1   0   0   8\n",
      "    0   0   0   0   0   4   0   0]\n",
      " [  0   0   0   0   1   0   0   3   0   0   1 144   0   2   2   0   1   1\n",
      "    2   0   0   0   0   2   0   0]\n",
      " [  1   4   0   2   0   1   0   2   0   0   0   1 149   1   1   1   0   3\n",
      "    0   0   1   0   5   0   1   0]\n",
      " [  1   0   0   0   0   0   0   4   0   0   0   1   1 114   1   0   2   3\n",
      "    0   0   0   1   4   0   2   0]\n",
      " [  0   1   2   3   0   0   6   1   0   0   0   1   0   0 100   2  15   4\n",
      "    0   0   3   0   4   0   0   0]\n",
      " [  0   1   0   1   3  10   1   2   0   3   0   0   0   1   0 140   0   1\n",
      "    0   0   0   0   2   0   0   0]\n",
      " [  0   1   0   0   1   0   0   5   0   1   1   2   0   0   7   1 108   9\n",
      "    0   0   3   0   0   4   1   1]\n",
      " [  0   5   0  11   0   0   0   5   0   0   3   0   1   4   4   0   0 114\n",
      "    1   0   0   0   0   0   0   1]\n",
      " [  0   3   2   1   0   3   0   4   0   1   2   3   0   0   0   5   3   0\n",
      "  111   1   0   0   0   2   0  13]\n",
      " [  0   2   0   0   1   7   0   0   1   0   4   0   0   0   3   0   0   3\n",
      "    1 146   1   0   0   5   3   0]\n",
      " [  0   0   0   0   2   4   1   5   0   0   3   0   6   3   3   0   2   2\n",
      "    0   0 127   0   1   1   0   0]\n",
      " [  0   3   0   0   0   0   2   2   0   0   2   0   0   1   1   1   0   0\n",
      "    0   0   2 129   4   0   6   0]\n",
      " [  0   0   0   0   0   0   1   5   0   0   0   0   5   1   0   2   1   0\n",
      "    0   0   2   1 120   0   3   0]\n",
      " [  0   2   0   4   5   1   0   7   2   0   6   1   0   0   1   3   0   4\n",
      "    5   2   0   0   0 126   2   2]\n",
      " [  1   0   0   3   0   1   0   1   0   0   0   0   0   0   0   3   1   1\n",
      "    1   7   2   0   1   0 127   5]\n",
      " [  0   1   0   2   3   4   3   1   0   0   1   1   0   0   0   1   0   1\n",
      "    2   0   0   0   0   1   3 119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.97      0.85      0.91       147\n",
      "           B       0.67      0.63      0.65       158\n",
      "           C       0.90      0.88      0.89       156\n",
      "           D       0.76      0.82      0.79       171\n",
      "           E       0.82      0.75      0.78       145\n",
      "           F       0.74      0.75      0.75       167\n",
      "           G       0.78      0.75      0.76       182\n",
      "           H       0.53      0.71      0.61       123\n",
      "           I       0.91      0.83      0.87       127\n",
      "           J       0.90      0.81      0.85       159\n",
      "           K       0.72      0.76      0.74       143\n",
      "           L       0.84      0.91      0.87       159\n",
      "           M       0.91      0.86      0.88       173\n",
      "           N       0.85      0.85      0.85       134\n",
      "           O       0.73      0.70      0.72       142\n",
      "           P       0.73      0.85      0.78       165\n",
      "           Q       0.72      0.74      0.73       145\n",
      "           R       0.61      0.77      0.68       149\n",
      "           S       0.78      0.72      0.75       154\n",
      "           T       0.90      0.82      0.86       177\n",
      "           U       0.88      0.79      0.83       160\n",
      "           V       0.98      0.84      0.91       153\n",
      "           W       0.82      0.85      0.84       141\n",
      "           X       0.78      0.73      0.75       173\n",
      "           Y       0.77      0.82      0.80       154\n",
      "           Z       0.79      0.83      0.81       143\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      4000\n",
      "   macro avg       0.80      0.79      0.80      4000\n",
      "weighted avg       0.80      0.79      0.80      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of using decision tree after distillation using decision tree:\",accuracy_score(yte1, ypred2))\n",
    "Y9=list(encoder.inverse_transform(yte1))\n",
    "Y10=list(encoder.inverse_transform(ypred2))\n",
    "print('The confusion Matrix is as Below:\\n')\n",
    "print(confusion_matrix(Y9,Y10,labels=list(encoder.classes_)))\n",
    "print(classification_report(Y9,Y10,target_names=list(encoder.classes_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
